<div class="demand__title">
  <h3><strong>Risikocheck</strong> für Algorithmen Information</h3>
</div>

<div class="demand__summary">
  <p>Wir fordern ein verpflichtendes Algorithmic Impact Assessment für alle Algorithmen, die potentiell in Grundrechte 
  eingreifen</p>
</div>
<div class="demand__detail">
  <p>
    Wir fordern ein verpflichtendes Algorithmic Impact Assessment 
    (eine sozio-technische Überprüfung von Algorithmen) vor Einführung. 
    Dies muss bei allen Algorithmen geschehen, die potentiell in Grundrechte 
    eingreifen.
  </p>
  <p>
    Automatisierte Systeme, die negative Konsequenzen für Menschen haben 
    könnten, müssen vor ihrer Einführung genau auf Funktionsweise und eventuelle 
    Folgewirkungen hin geprüft werden.
  </p>
  <p>
    Hier gibt es internationale Best-Practices der Kontrolle von Algorithmen, 
    ein sogenanntes Algorithmic Impact Assessment. Dies vorzunehmen wurde beim 
    AMS-Algorithmus verabsäumt und muss zukünftig verpflichtend sein.
  </p>
  <p>
    Algorithmen müssen auf Fehler im System geprüft werden, Biases in den Daten 
    und dadurch resultierende negative Feedback Zyklen müssen analysiert werden, 
    bevor solche Systeme gestartet und flächendeckend eingesetzt werden. 
  </p>
  <p>
    Nicht jede Statistische Regression folgt dem Prinzip von Ursache und 
    Wirkung. Negative Effekte auf Menschen aufgrund von Fehlanalysen müssen 
    vermieden werden. 
  </p>
  <p>
    Deswegen fordern wir ein Algorithmic Impact Assessment, wie es bereits 
    <a href="https://www.canada.ca/en/government/system/digital-government/modern-emerging-technologies/responsible-use-ai/algorithmic-impact-assessment.html" target="_blank">Canada</a> 
    oder die 
    <a href="https://www.theverge.com/2019/4/15/18309437/new-york-city-accountability-task-force-law-algorithm-transparency-automation" target="_blank">Stadt New York</a> 
    eingeführt haben.
  </p>
</div>