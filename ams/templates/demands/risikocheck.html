<div class="demand__title">
  <h3>Risikocheck für Algorithmen</h3>
</div>

<div class="demand__summary">
  <p>Wir fordern eine Überprüfung von Algorithmen auf deren Technik und soziale Auswirkungen, bevor diese eingeführt werden (Verpflichtung für ein vorgelagertes Algorithmic Impact Assessment). </p>
</div>
<div class="demand__detail">
  <p>
    Automatisierte Systeme, die negative Konsequenzen für Menschen haben könnten, müssen vor ihrer Einführung genau auf Funktionsweise und eventuelle Folgewirkungen hin geprüft werden. Dies muss bei allen Algorithmen geschehen, die potentiell in Grundrechte eingreifen
  </p>
   <p>
    Hier gibt es internationale Best-Practices der Kontrolle von Algorithmen, 
    ein sogenanntes Algorithmic Impact Assessment. Dies vorzunehmen wurde beim 
    AMS-Algorithmus verabsäumt und muss zukünftig verpflichtend sein.
  </p>
  <p>
    Algorithmen müssen auf Fehler im System geprüft werden, Biases in den Daten 
    und daraus resultierende negative Feedback Zyklen müssen analysiert werden, 
    bevor solche Systeme gestartet und flächendeckend eingesetzt werden. 
  </p>
  <p>
    Nicht jede Statistische Regression folgt dem Prinzip von Ursache und 
    Wirkung. Negative Effekte auf Menschen aufgrund von Fehlanalysen müssen 
    vermieden werden. 
  </p>
  <p>
    Deswegen fordern wir ein Algorithmic Impact Assessment, wie es bereits 
    <a href="https://www.canada.ca/en/government/system/digital-government/modern-emerging-technologies/responsible-use-ai/algorithmic-impact-assessment.html" target="_blank">Kanada</a> 
    oder die 
    <a href="https://www.theverge.com/2019/4/15/18309437/new-york-city-accountability-task-force-law-algorithm-transparency-automation" target="_blank">Stadt New York</a> 
    eingeführt haben.
  </p>
</div>